# EVA + Ollama + LocalTunnel

## ‚ú® Objetivo
Configurar un entorno local donde EVA (agente conversacional) utilice el modelo LLaMA 3 servido por Ollama y se exponga v√≠a LocalTunnel para poder ser consumido desde un backend desplegado en Render.

---

## ‚úÖ Requisitos

- [x] Tener instalado [Ollama](https://ollama.com/)
- [x] Tener `node` y `npm` instalados
- [x] Tener `git`
- [x] Tener `eva_llama_14.py` configurado
- [x] Conexi√≥n a GitHub (deploy en Render)

---

## üöÄ Paso a paso

### 1. Iniciar el servidor de Ollama

```bash
set OLLAMA_HOST=0.0.0.0  # En Windows CMD
ollama serve
```

Esto hace que Ollama acepte conexiones externas (necesario para LocalTunnel).

---

### 2. Instalar LocalTunnel (una vez)

```bash
npm install -g localtunnel
```

Si no ten√©s permisos, pod√©s usar `npx localtunnel` directamente.

---

### 3. Crear el t√∫nel y obtener la URL

```bash
lt --port 11434
```

Obtendr√°s una URL como:

```
https://shiny-penguin.loca.lt
```

---

### 4. Actualizar `eva_llama_14.py`

Busc√° la parte donde se define la URL de Ollama:

```python
CONFIG["ollama_api_url"] = "https://anterior-url.loca.lt/api/generate"
```

Y reemplazala por la nueva URL:

```python
CONFIG["ollama_api_url"] = "https://shiny-penguin.loca.lt/api/generate"
```

---

### 5. Subir los cambios a GitHub

```bash
git add eva_llama_14.py
git commit -m "Actualizar URL de Ollama"
git push origin main
```

Render har√° el redeploy autom√°ticamente y EVA usar√° la nueva instancia de Ollama.

---

## ü§ñ Automatizar el flujo

Crea un archivo `actualizar_ollama_url.sh`:

```bash
#!/bin/bash

export OLLAMA_HOST=0.0.0.0
ollama serve &

sleep 3

url=$(npx localtunnel --port 11434 | grep -Eo "https://[a-zA-Z0-9\-]+\.loca\.lt")

sed -i '' "s|https://.*\.loca\.lt/api/generate|$url/api/generate|g" eva_llama_14.py

git add eva_llama_14.py
git commit -m "Actualizar URL del t√∫nel Ollama"
git push origin main

echo "[‚úÖ] Nuevo URL aplicado: $url"
```

Hacelo ejecutable:

```bash
chmod +x actualizar_ollama_url.sh
```

Ejecutalo cuando quieras actualizar el t√∫nel:

```bash
./actualizar_ollama_url.sh
```

---

## üß™ Prueba en Postman

**Endpoint EVA (Render):**
```
https://eva-llama-backend.onrender.com/chat
```

**Body:**
```json
{
  "message": "Hola EVA, ¬øqu√© pod√©s hacer?"
}
```

**Header:**
```
Content-Type: application/json
```

---

## ‚úÖ Resultado

Tu backend EVA estar√° vinculado din√°micamente con Ollama local, incluso cuando LocalTunnel reinicie su URL.

